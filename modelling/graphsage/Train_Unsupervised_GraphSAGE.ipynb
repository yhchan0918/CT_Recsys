{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9801cc50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-24 2022-10-23\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8578b41d059b49fa91c9b848e7eb32a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.0167176583333332, max=1.0))â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/yhchan/Downloads/FYP/CT_Recsys/modelling/graphsage/wandb/run-20230426_192125-loeuxrr8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yhchan0918/Experimenting_GraphSAGE/runs/loeuxrr8' target=\"_blank\">dazzling-firebrand-35</a></strong> to <a href='https://wandb.ai/yhchan0918/Experimenting_GraphSAGE' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/yhchan0918/Experimenting_GraphSAGE' target=\"_blank\">https://wandb.ai/yhchan0918/Experimenting_GraphSAGE</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/yhchan0918/Experimenting_GraphSAGE/runs/loeuxrr8' target=\"_blank\">https://wandb.ai/yhchan0918/Experimenting_GraphSAGE/runs/loeuxrr8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-26 19:21:41.948 | INFO     | process_data:main_train_test:140 - Split df into train and test portion\n",
      "/Users/yhchan/Downloads/FYP/CT_Recsys/modelling/graphsage/process_data.py:172: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1670525682339/work/torch/csrc/utils/tensor_numpy.cpp:205.)\n",
      "  temp = torch.from_numpy(val).view(-1, 1).to(torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole Graph HeteroData(\n",
      "  \u001b[1mlisting\u001b[0m={ x=[18523, 158] },\n",
      "  \u001b[1muser\u001b[0m={ x=[394551, 384] },\n",
      "  \u001b[1m(user, rates, listing)\u001b[0m={\n",
      "    edge_index=[2, 408596],\n",
      "    edge_label=[408596],\n",
      "    edge_label_index=[2, 408596]\n",
      "  },\n",
      "  \u001b[1m(listing, rev_rates, user)\u001b[0m={ edge_index=[2, 408596] }\n",
      ")\n",
      "Training Heterogenous Graph HeteroData(\n",
      "  \u001b[1mlisting\u001b[0m={ x=[17229, 158] },\n",
      "  \u001b[1muser\u001b[0m={ x=[324135, 384] },\n",
      "  \u001b[1m(user, rates, listing)\u001b[0m={\n",
      "    edge_index=[2, 334678],\n",
      "    edge_label=[334678],\n",
      "    edge_label_index=[2, 334678]\n",
      "  },\n",
      "  \u001b[1m(listing, rev_rates, user)\u001b[0m={ edge_index=[2, 334678] }\n",
      ")\n",
      "Test Heterogenous Graph HeteroData(\n",
      "  \u001b[1mlisting\u001b[0m={ x=[14380, 158] },\n",
      "  \u001b[1muser\u001b[0m={ x=[72447, 384] },\n",
      "  \u001b[1m(user, rates, listing)\u001b[0m={\n",
      "    edge_index=[2, 73918],\n",
      "    edge_label=[73918],\n",
      "    edge_label_index=[2, 73918]\n",
      "  },\n",
      "  \u001b[1m(listing, rev_rates, user)\u001b[0m={ edge_index=[2, 73918] }\n",
      ")\n",
      "Test Heterogenous Graph (Cold Start Scenerio) HeteroData(\n",
      "  \u001b[1mlisting\u001b[0m={ x=[14254, 158] },\n",
      "  \u001b[1muser\u001b[0m={ x=[70416, 384] },\n",
      "  \u001b[1m(user, rates, listing)\u001b[0m={\n",
      "    edge_index=[2, 71775],\n",
      "    edge_label=[71775],\n",
      "    edge_label_index=[2, 71775]\n",
      "  },\n",
      "  \u001b[1m(listing, rev_rates, user)\u001b[0m={ edge_index=[2, 71775] }\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./train)... Done. 5.1s\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./test)... Done. 2.9s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<wandb.sdk.wandb_artifacts.Artifact at 0x336c4ed60>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "import pandas as pd\n",
    "from loguru import logger\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from datetime import datetime as dt\n",
    "import os\n",
    "from dateutil.relativedelta import relativedelta  # type: ignore\n",
    "import functools\n",
    "\n",
    "from process_data import *\n",
    "from constants import *\n",
    "from unsup_model import *\n",
    "from evaluate import *\n",
    "\n",
    "def get_entity2dict(df, id_col):\n",
    "    entity2dict = {}\n",
    "\n",
    "    for idx, _id in enumerate(df[id_col].to_list()):\n",
    "        entity2dict[_id] = idx\n",
    "\n",
    "    return entity2dict\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "start_date = dt.strptime(\"2021-10-24\", \"%Y-%m-%d\").date()\n",
    "if start_date == dt.strptime(MAX_START_DATE, \"%Y-%m-%d\").date():\n",
    "    raise Exception(\"Stop Simulation\")\n",
    "end_date, nxt_start_date = split_date_by_period_months(start_date, TOTAL_MONTHS_PER_ITERATION)\n",
    "print(start_date, end_date)\n",
    "directory = \"/Users/yhchan/Downloads/FYP/data/processed\"\n",
    "reviews = pd.read_parquet(f\"{directory}/reviews_with_interactions.parquet\")\n",
    "listings = pd.read_parquet(f\"{directory}/listings_with_interactions.parquet\")\n",
    "\n",
    "config = {\n",
    "        \"architecture\": \"Unsupervised GraphSAGE\",\n",
    "        \"start_date\": start_date,\n",
    "        \"end_date\": end_date,\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"hidden_channels\": 64,\n",
    "        \"train_batch_size\": 128,\n",
    "        \"test_batch_size\": 128,\n",
    "        \"epochs\": 50,\n",
    "        \"train_num_neighbours\": [10, 10],\n",
    "        \"test_num_neighbours\": [-1],\n",
    "        \"train_split_period_months\": 10,\n",
    "        \"total_months_of_data\": TOTAL_MONTHS_PER_ITERATION,\n",
    "        \"rec_K\":10\n",
    "\n",
    "    }\n",
    "\n",
    "wandb.init(\n",
    "    project=PROJECT_NAME,\n",
    "    config=config,\n",
    ")\n",
    "wandb.define_metric(\"train_loss\", step_metric=\"epoch\", summary=\"min\")\n",
    "wandb.define_metric(\"test_loss\", step_metric=\"epoch\", summary=\"min\")\n",
    "\n",
    "# Split into train, test and test for cold start scenario\n",
    "(\n",
    "    train_reviews,\n",
    "    train_listings,\n",
    "    train_reviewers,\n",
    "    test_reviews,\n",
    "    test_listings,\n",
    "    test_reviewers,\n",
    ") = main_train_test(\n",
    "    reviews,\n",
    "    listings,\n",
    "    start_date,\n",
    "    end_date,\n",
    "    config[\"train_split_period_months\"],\n",
    ")\n",
    "\n",
    "cold_start_test_reviews = filter_test_data_by_scenario(\n",
    "    train_reviews, test_reviews, \"reviewer_id\", \"cold_start_new_user\"\n",
    ")\n",
    "cold_start_test_listings, cold_start_test_reviewers = build_partitioned_data(\n",
    "    cold_start_test_reviews, listings\n",
    ")\n",
    "# Build Graph\n",
    "involved_reviews = pd.concat([train_reviews, test_reviews])\n",
    "involved_listings, involved_reviewers = build_partitioned_data(involved_reviews, listings)\n",
    "involved_data = build_heterograph(involved_reviews, involved_listings, involved_reviewers, True)\n",
    "train_data = build_heterograph(train_reviews, train_listings, train_reviewers, True)\n",
    "test_data = build_heterograph(test_reviews, test_listings, test_reviewers, True)\n",
    "cold_start_test_data = build_heterograph(cold_start_test_reviews, cold_start_test_listings, cold_start_test_reviewers, True)\n",
    "print(\"Whole Graph\", involved_data)\n",
    "print(\"Training Heterogenous Graph\", train_data)\n",
    "print(\"Test Heterogenous Graph\", test_data)\n",
    "print(\"Test Heterogenous Graph (Cold Start Scenerio)\", cold_start_test_data)\n",
    "\n",
    "involved_listings2dict = get_entity2dict(involved_listings, \"listing_id\")\n",
    "reverse_involved_listings2dict = {k: v for v, k in involved_listings2dict.items()}\n",
    "\n",
    "metadata_dict = {\n",
    "    \"num_reviews\": len(involved_reviews),\n",
    "    \"num_train_reviews\": len(train_reviews),\n",
    "    \"num_test_reviews\": len(test_reviews),\n",
    "    \"num_cold_start_test_reviews\":len(cold_start_test_reviews),\n",
    "    \n",
    "    \"num_unique_listings\": len(involved_listings),\n",
    "    \"num_unique_train_listings\": len(train_listings),\n",
    "    \"num_unique_test_listings\": len(test_listings),\n",
    "    \"num_unique_cold_start_test_listings\":len(cold_start_test_listings),\n",
    "    \n",
    "    \"num_unique_reviewers\": len(involved_reviewers),\n",
    "    \"num_unique_train_reviewers\": len(train_reviewers),\n",
    "    \"num_unique_test_reviewers\": len(test_reviewers),\n",
    "    \"num_unique_cold_start_test_reviewers\":len(cold_start_test_reviewers),\n",
    "\n",
    "}\n",
    "\n",
    "wandb.log(metadata_dict)\n",
    "train_reviews.to_parquet(\"train/train_reviews.parquet\", index=False)\n",
    "train_listings.to_parquet(\"train/train_listings.parquet\", index=False)\n",
    "train_reviewers.to_parquet(\"train/train_reviewers.parquet\", index=False)\n",
    "test_reviews.to_parquet(\"test/test_reviews.parquet\", index=False)\n",
    "test_listings.to_parquet(\"test/test_listings.parquet\", index=False)\n",
    "test_reviewers.to_parquet(\"test/test_reviewers.parquet\", index=False)\n",
    "cold_start_test_reviews.to_parquet(\"test/cold_start_test_reviews.parquet\", index=False)\n",
    "cold_start_test_listings.to_parquet(\"test/cold_start_test_listings.parquet\", index=False)\n",
    "cold_start_test_reviewers.to_parquet(\"test/cold_start_test_reviewers.parquet\", index=False)\n",
    "\n",
    "dataset_art = wandb.Artifact(f\"{start_date}_{end_date}_data\", type=\"dataset\")\n",
    "for dir in [\"train\", \"test\"]:\n",
    "    dataset_art.add_dir(dir)\n",
    "wandb.log_artifact(dataset_art)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d44c477d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-26 19:22:54.819 | INFO     | __main__:<module>:87 - Epoch: 001, Train Loss: 2.0640, Test Loss: 0.6423 \n",
      "2023-04-26 19:23:21.718 | INFO     | __main__:<module>:87 - Epoch: 002, Train Loss: 0.6083, Test Loss: 0.5620 \n",
      "2023-04-26 19:23:47.234 | INFO     | __main__:<module>:87 - Epoch: 003, Train Loss: 1.0001, Test Loss: 1.1732 \n",
      "2023-04-26 19:24:12.500 | INFO     | __main__:<module>:87 - Epoch: 004, Train Loss: 1.0541, Test Loss: 1.7503 \n",
      "2023-04-26 19:24:40.314 | INFO     | __main__:<module>:87 - Epoch: 005, Train Loss: 1.0036, Test Loss: 1.8673 \n",
      "2023-04-26 19:25:05.544 | INFO     | __main__:<module>:87 - Epoch: 006, Train Loss: 1.0052, Test Loss: 1.7289 \n",
      "2023-04-26 19:25:34.652 | INFO     | __main__:<module>:87 - Epoch: 007, Train Loss: 1.0057, Test Loss: 1.5686 \n",
      "2023-04-26 19:26:02.474 | INFO     | __main__:<module>:87 - Epoch: 008, Train Loss: 1.0051, Test Loss: 1.3469 \n",
      "2023-04-26 19:26:27.570 | INFO     | __main__:<module>:87 - Epoch: 009, Train Loss: 1.3591, Test Loss: 1.3173 \n",
      "2023-04-26 19:26:52.751 | INFO     | __main__:<module>:87 - Epoch: 010, Train Loss: 1.3591, Test Loss: 1.3162 \n",
      "2023-04-26 19:27:17.297 | INFO     | __main__:<module>:87 - Epoch: 011, Train Loss: 1.3591, Test Loss: 1.3094 \n",
      "2023-04-26 19:27:43.386 | INFO     | __main__:<module>:87 - Epoch: 012, Train Loss: 1.3591, Test Loss: 1.2919 \n",
      "2023-04-26 19:28:10.797 | INFO     | __main__:<module>:87 - Epoch: 013, Train Loss: 1.3591, Test Loss: 1.2671 \n",
      "2023-04-26 19:28:36.520 | INFO     | __main__:<module>:87 - Epoch: 014, Train Loss: 1.3591, Test Loss: 1.2959 \n",
      "2023-04-26 19:29:00.832 | INFO     | __main__:<module>:87 - Epoch: 015, Train Loss: 1.3591, Test Loss: 1.3092 \n",
      "2023-04-26 19:29:26.683 | INFO     | __main__:<module>:87 - Epoch: 016, Train Loss: 1.3591, Test Loss: 1.3081 \n",
      "2023-04-26 19:29:53.005 | INFO     | __main__:<module>:87 - Epoch: 017, Train Loss: 1.3591, Test Loss: 1.3192 \n",
      "2023-04-26 19:30:17.116 | INFO     | __main__:<module>:87 - Epoch: 018, Train Loss: 1.3591, Test Loss: 1.3221 \n",
      "2023-04-26 19:30:42.380 | INFO     | __main__:<module>:87 - Epoch: 019, Train Loss: 1.3591, Test Loss: 1.3130 \n",
      "2023-04-26 19:31:07.909 | INFO     | __main__:<module>:87 - Epoch: 020, Train Loss: 1.3591, Test Loss: 1.3110 \n",
      "2023-04-26 19:31:33.372 | INFO     | __main__:<module>:87 - Epoch: 021, Train Loss: 1.3591, Test Loss: 1.3069 \n",
      "2023-04-26 19:31:59.839 | INFO     | __main__:<module>:87 - Epoch: 022, Train Loss: 1.3591, Test Loss: 1.3048 \n",
      "2023-04-26 19:32:26.615 | INFO     | __main__:<module>:87 - Epoch: 023, Train Loss: 1.3591, Test Loss: 1.3012 \n",
      "2023-04-26 19:32:51.964 | INFO     | __main__:<module>:87 - Epoch: 024, Train Loss: 1.3591, Test Loss: 1.2974 \n",
      "2023-04-26 19:33:17.572 | INFO     | __main__:<module>:87 - Epoch: 025, Train Loss: 1.3591, Test Loss: 1.2930 \n",
      "2023-04-26 19:33:43.244 | INFO     | __main__:<module>:87 - Epoch: 026, Train Loss: 1.3591, Test Loss: 1.2884 \n",
      "2023-04-26 19:34:09.395 | INFO     | __main__:<module>:87 - Epoch: 027, Train Loss: 1.3591, Test Loss: 1.2789 \n",
      "2023-04-26 19:34:35.128 | INFO     | __main__:<module>:87 - Epoch: 028, Train Loss: 1.3591, Test Loss: 1.2819 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 69\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, wandb\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     68\u001b[0m     model_is_best \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m     test_loss \u001b[38;5;241m=\u001b[39m test_wrapper(model)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m train_loss \u001b[38;5;241m<\u001b[39m best_train_loss:\n",
      "Cell \u001b[0;32mIn[2], line 28\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Make predictions for this batch\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m h_src \u001b[38;5;241m=\u001b[39m h[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m][batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisting\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39medge_label_index[\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m     30\u001b[0m h_dst \u001b[38;5;241m=\u001b[39m h[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisting\u001b[39m\u001b[38;5;124m\"\u001b[39m][batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisting\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39medge_label_index[\u001b[38;5;241m1\u001b[39m]]\n",
      "File \u001b[0;32m~/miniconda3/envs/tt/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Downloads/FYP/CT_Recsys/modelling/graphsage/unsup_model.py:26\u001b[0m, in \u001b[0;36mUnsupervised_Model.forward\u001b[0;34m(self, x_dict, edge_index_dict)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_dict, edge_index_dict):\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# Node embedding here\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index_dict\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tt/lib/python3.8/site-packages/torch/fx/graph_module.py:658\u001b[0m, in \u001b[0;36mGraphModule.recompile.<locals>.call_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_wrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 658\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrapped_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tt/lib/python3.8/site-packages/torch/fx/graph_module.py:267\u001b[0m, in \u001b[0;36m_WrappedCall.__call__\u001b[0;34m(self, obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls_call(obj, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 267\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m e\u001b[38;5;241m.\u001b[39m__traceback__\n",
      "File \u001b[0;32m~/miniconda3/envs/tt/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m<eval_with_key>.1:12\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index)\u001b[0m\n\u001b[1;32m     10\u001b[0m edge_index__listing__rev_rates__user \u001b[38;5;241m=\u001b[39m edge_index_dict\u001b[38;5;241m.\u001b[39mget((\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlisting\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrev_rates\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;28;01mNone\u001b[39;00m);  edge_index_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     11\u001b[0m conv1__listing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1\u001b[38;5;241m.\u001b[39muser__rates__listing((x__user, x__listing), edge_index__user__rates__listing)\n\u001b[0;32m---> 12\u001b[0m conv1__user \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlisting__rev_rates__user\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx__listing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx__user\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index__listing__rev_rates__user\u001b[49m\u001b[43m)\u001b[49m;  x__listing \u001b[38;5;241m=\u001b[39m x__user \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     13\u001b[0m relu__listing \u001b[38;5;241m=\u001b[39m conv1__listing\u001b[38;5;241m.\u001b[39mrelu();  conv1__listing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     14\u001b[0m relu__user \u001b[38;5;241m=\u001b[39m conv1__user\u001b[38;5;241m.\u001b[39mrelu();  conv1__user \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tt/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/tt/lib/python3.8/site-packages/torch_geometric/nn/conv/sage_conv.py:132\u001b[0m, in \u001b[0;36mSAGEConv.forward\u001b[0;34m(self, x, edge_index, size)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# propagate_type: (x: OptPairTensor)\u001b[39;00m\n\u001b[1;32m    131\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpropagate(edge_index, x\u001b[38;5;241m=\u001b[39mx, size\u001b[38;5;241m=\u001b[39msize)\n\u001b[0;32m--> 132\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlin_l\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m x_r \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_weight \u001b[38;5;129;01mand\u001b[39;00m x_r \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/tt/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/tt/lib/python3.8/site-packages/torch_geometric/nn/dense/linear.py:136\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    132\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;124;03m        x (Tensor): The features.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Modelling\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_data = train_data.to(device)\n",
    "train_loader = prepare_data_loader(\n",
    "    data=train_data,\n",
    "    batch_size=config[\"train_batch_size\"],\n",
    "    num_neighbours=config[\"train_num_neighbours\"],\n",
    ")\n",
    "test_loader = prepare_data_loader(\n",
    "    data=test_data,\n",
    "    batch_size=config[\"test_batch_size\"],\n",
    "    num_neighbours=config[\"test_num_neighbours\"],\n",
    ")\n",
    "model = Unsupervised_Model(hidden_channels=config[\"hidden_channels\"], data=involved_data).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "\n",
    "def train():\n",
    "    model.train(True)\n",
    "    total_loss = 0\n",
    "    # Why using mini-batch gradient descent\n",
    "    # Update NN multiple times every epoch, Make more precise update to the parameters by calculating the average loss in each step\n",
    "    # Reduce overall training time and num of required epochs for reaching convergence, computational efficiency\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        # Zero gradients for every batch\n",
    "        optimizer.zero_grad()\n",
    "        # Make predictions for this batch\n",
    "        h = model(batch.x_dict, batch.edge_index_dict)\n",
    "        h_src = h[\"user\"][batch[\"user\", \"listing\"].edge_label_index[0]]\n",
    "        h_dst = h[\"listing\"][batch[\"user\", \"listing\"].edge_label_index[1]]\n",
    "        pred = (h_src * h_dst).sum(dim=-1)\n",
    "        # Compute the loss and its gradients\n",
    "        loss = F.binary_cross_entropy_with_logits(pred, batch[\"user\", \"listing\"].edge_label)\n",
    "        loss.backward()\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "        total_loss += float(loss) * pred.size(0)\n",
    "\n",
    "    train_loss = total_loss / train_data.num_nodes\n",
    "    return train_loss\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(test_data_loader, test_data, model):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    for batch in test_data_loader:\n",
    "        batch = batch.to(device)\n",
    "        # Make predictions for this batch\n",
    "        h = model(batch.x_dict, batch.edge_index_dict)\n",
    "        h_src = h[\"user\"][batch[\"user\", \"listing\"].edge_label_index[0]]\n",
    "        h_dst = h[\"listing\"][batch[\"user\", \"listing\"].edge_label_index[1]]\n",
    "        pred = (h_src * h_dst).sum(dim=-1)\n",
    "        # Compute the loss and its gradients\n",
    "        loss = F.binary_cross_entropy_with_logits(pred, batch[\"user\", \"listing\"].edge_label)\n",
    "        total_loss += float(loss) * pred.size(0)\n",
    "\n",
    "    test_loss = total_loss / test_data.num_nodes\n",
    "    return test_loss\n",
    "\n",
    "\n",
    "best_train_loss = float(\"inf\")\n",
    "best_test_loss = float(\"inf\")\n",
    "model_prefix = \"./unsupervised_models\"\n",
    "\n",
    "# Train and Evaluate Loss\n",
    "test_wrapper = functools.partial(test, test_loader, test_data)\n",
    "for epoch in range(1, wandb.config[\"epochs\"] + 1):\n",
    "    model_is_best = False\n",
    "    train_loss = train()\n",
    "    test_loss = test_wrapper(model)\n",
    "\n",
    "    if train_loss < best_train_loss:\n",
    "        wandb.run.summary[\"best_train_loss\"] = train_loss\n",
    "        best_train_loss = train_loss\n",
    "\n",
    "    if test_loss < best_test_loss:\n",
    "        wandb.run.summary[\"best_test_loss\"] = test_loss\n",
    "        best_test_loss = test_loss\n",
    "        model_is_best = True\n",
    "\n",
    "    metrics_dict = {\n",
    "        \"train_loss\": train_loss,\n",
    "        \"test_loss\": test_loss,\n",
    "        \"epoch\": epoch,\n",
    "    }\n",
    "    wandb.log(metrics_dict)\n",
    "    logger.info(\n",
    "        f\"Epoch: {epoch:03d}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f} \"\n",
    "    )\n",
    "    \n",
    "    model_path = f\"{model_prefix}/{epoch}_model_state_dict.pt\"\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    model_art = wandb.Artifact(f\"{MODEL_NAME}_epoch_epoch\", type=\"model\")\n",
    "    model_art.add_file(model_path)\n",
    "    wandb.log_artifact(\n",
    "        model_art,\n",
    "        aliases=[\n",
    "            \"BEST\",\n",
    "        ]\n",
    "        if model_is_best\n",
    "        else None,\n",
    "    )\n",
    "        \n",
    "logger.info(\"End of Training\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tt] *",
   "language": "python",
   "name": "conda-env-tt-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
